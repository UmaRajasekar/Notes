There are two types of back-up for AWS: 1. Automated back-ups and Database Snapshots
Automated backup allow you to recover your database to any point in time within a "retention period". The retention period  can be between one and 35 days. Automated backups will take a full daily snapshot and will also store transaction logs throughout the day. When you do a recovery, AWS first choose the most recent daily backup, and then apply transaction logs relavent to that day. This allows you to do a point in time recovery down to second, within the retention period.
Automated Backups
Automated backups are enabled by default. The backup data is stored in S3 and you get  free storage space equal to the size of your database. So if you have an RDS instance of 10Gb, you will get 10GB worth of storage.
Backups are taken within a defined window. During the backup window, storage I/O may be suspended while your data is being backed up and you may experience elavated latency.
Snapshots
DB Snapshots are done manually. They are stored even after you delete the original RDS instance, unlike automated backups.
Restoration Backups
Whenever you restore either an automatic backup or a manual snapshot, the restoration version of the database will be a new RDS instance with a new DNS end-point.
Encryption at rest is supported for MySQL, Oracle, SQL server, PostgreSQL, MariaDB and Aurora. Encryption is done using the AWS Key management service(KMS) service. Once your RDS instance is encrypted, the data stored at rest in the underlying storage is en-ncrypted, as are its automated backups, read replicas and snapshots.
At the present time, encrypting an existing DB instance is not supported. To use Amazon RDS encryption for an existing database, you must create a snapshot, make a copy of that snapshot and encrypt the copy.

Multi AZ.
This is used for disaster recovery. When multi ec2instance connected to db and performs action on db1, a same copy of db1 in other location is changed and all the action of db1 is replicated to db2. When db1 fails, it automatically failover to db2.
Allows you to have exact copy of prod database in another availablity zone.
AWS handles replication, so when prod database written, this write will auto sync to stand by database.
In the even of planned database maintence, db instance failure and an availability zone failure, amazon RDS will automatically failover to the standby so the database operations can resume quickly without administrative intervention.
Read Replica:-
This is used for performance improvement.
Scaling out database by taking load out of database and spread it across
Allows you to have read-only copy of prod database.
This is achived by asynchronous replication from the primary RDS instance to the read replica. 
you can use read replicas primarily for very read-heavy database workloads.

DynamoDB
Fast and fexible NoSQL database service for all the applications that need consistent , single digit millisecond latency at any scale. It is fully managed database and supports both document and key-value data models.
Its flexible data model and reliable performance make it a great fit for mobile, web,gaming, ad-hoc and IOT and many other applications.
Stored on SSD storage
Spread across 3 geographically distinct data centers
Eventually Consistent Read
-> Consistency across all copies of data is usually reached within a second. Repeating a read after a short time should return the updated data.
Strong Consistent Read
-> A strong consistent read returns a result that refelects all writes that recieved a successful response prior to the read.
DynamoDB pricing
Provisioned ThroughPut Capacity
-> Write Throughput $0.0065 per hour for every 10 units
-> Read Throughput $0.0065 per hour for every 50 units
Storage cost of $0.25gb per month.
Advantage:-
We can add fields whenever required
Read and capacity of DB can be changed on the fly without having any downtime. Push button scaling.
Amazon Redshift
Fast,powerful, fully managed, perabyte-scale data warehouse service in the cloud.
Cost effective
Data warehousing databases use different type of architecture both from the database perspective and infrastructure layer.
Redshift configuration
Single Node
Multi Node
-> Leader Node(manages client connections and perform queries and computation).
-> Compute Node(stores data and perform queries and computation). Upto 128 compute nodes.
Columnar Data Storage.
Instead of storing datas as series of rows, Amazon Redshift organizes the data by column. 
unlike row based systems, which are ideal for transactional processing,column-based systems are ideal for data warehousing and analytics, where queries often involve aggregates performed over large data sets. Since only the columns involved in the queries are processed and columnar data is stored sequentially on the storage media,column-based systems require far fewer I/Os, greatly improving query performance.
Advanced Compression:
Columnar data stores can be compressed much more than row-based data stores because similar data is stored sequencially on disk. Amazon Redshift employs multiple compression techniques and can often achieve significant compression relative to traditional relational data stores. In addition, Amazon redshift does not require indexes or materialize views and so uses less space than traditional relative database systems. When loading data into an empty table, Amazon Redshift automatically samples your data and selects the most appropriate compression scheme.
Massive Parallel Processing:
Amazon Redshift automatically distributes data and query loads across all nodes. Amazon Redshift makes it easy to add nodes to your datawarehouse and enables to maintain faster query performance as your data warehouse grows.
Redshift Pricing:-
Compute Node Hours{Only compute nodes are charged and leader node are not charged}
BackUp
Data Transfer within VPC and not outside it
Redshift Security
Encrypted in transit using SSL
Encryption at rest using AES-256 encryption
Redshift takes care of key management. We can also manage your keys using hardware security module and AWS KMS.
Only available in 1 AZ
Can restore snapshots to new AZ's in the event of outage.

Elaticache
It is a web service that makes it easy to deploy, operate and scale an in memory cache in the cloud. 
This service improves performance of web application from fast, managed, in memory caches, instead of relying entirely on slower disk-based databases.
It can be used to significantly improve latency and throughput for many read-heavy application workloads(such as social networking, gaming, media sharing and Q&A portals) or compute-intensive workloads(such as recommendation engine)

Cache improves application performance by storing critical pieces of data in memory for low-latency access. Cached information may include the results of I/O-intensive database queries or the results of computationally-intensive calculations.
Memcached
-> A widely adopted memory object caching system. Elasticache is protocol compliant with Memcached, so popular tool that you use today with existing Memcached environments will work seamlessly with the service.
Redis
A popular open-source in-memory key-value store that supports data structure such as sets and lists. Elasticache supports master/slave replication and multi-AZ which can be used to achieve cross AZ redundancy.
Aurora
Aurora is the database engine that is available in RDS
It is a mysql compatible, relational database engine that combines speed and availability of high end commercial databases with the simplicity and cost-effectiveness of open source databases. Amazon aurora provides upto five times better performance at a price point one tenth that of a commercial database while delivering similar performance and availability
Autoscaling of 10gb to 64tb
Compute resorce scale upto 244gb and upto 32vCPU
No downtime while scaling
Copies two copies of db in each availability zone in atleast 3AZ.
Can handle loss of 2 copy without impacting write operation
Can handle loss of 3 copy without impacting read operation
Storage is self-healing
Two types of Replicas available
-> Aurora replicas upto 15
-> mySQL read replicas upto 5	
DynamoDB has push button scaling, meaning that you can scale your database on the fly,without any downtime.

https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html#USER_PIOPS


VPC-Virtual Private Cloud.
Virtual Data Center in the cloud.
Amazon VPC lets you provision a logically isolated section of the Amazon Web services cloud where you can launch AWS resource in a virtual network you define. 
You have control of your virtual networking environment including selection of your own IP address range, creation of subnets and configuration of route tables and network gateways.
You can easily customize the network configuration for your Amazon Virtual Private Cloud. For example, we can create a public-facing subnet for your webservers that has access to the Internet, and place your backend systems such as databases or application servers in a private facing subnet with no internet access. You can leverage multiple layer of security, including security groups and network access control lists to help control access to Amazon ec2 instances in the subnet.
You can also create hardware virtual private network connection between your corporate datacenter and your VPC and leverage the AWS cloud as the extension of corporate datacenter.
Region-> VPC-> Internet gateway/VPC gateway -> Route table -> Network ACL-> (subnets)Security Group -> Instance
Launch instances of subnet of your choosing
Assign custom IP address in each subnets
Configure route tables between subnets
Create internet gateway and attach it to VPC
Much better security control over AWS resources. ACL can be used to block specific IP addresses.
Instance security group. Security groups can span across different availability zone.
Default VPC vs Custom VPC
Default VPC is user friendly, allowing you to immediately deploy instances.
All subnets in the default VPC is internet accessible.
Each ec2-instance has both public and private IP address whereas in custom VPC, we will have only private IP address.
VPC Peering:-
Allows you to connect one VPC to another via direct network route using private IP address.
Instances behave as they are in the same private network.
We can peer VPC with other AWS accounts as well as with other VPCs in the same account.
VPC peering is always a star model
1 subnet=1 availability zone
Security Group are statful and ACL are stateless
No transitive peering
1VPC = 1 Internet Gateway
Security Group is only for that particular VPC
Router links subnet with gateway

Security Group spans within the VPC.

Network Address Translator
-> This can be used as bridge to connect ec2 instance in private subnet and other ec2 instance in public subnet.
-> When creating a NAT instance, you need to disble source/destination check on the instance.
-> NAT instance must be in a public subnet
-> There must be a route table out of the private subnet to the NAT instance, in order for this to work.
-> The amount of traffic that NAT instances can support depends on the instance size. If you are bottlenecking, increase the instance size.
-> NAT instance behind security group.

NAT Gateways
-> Preferred by the enterprise
-> Scale automatically upto 10Gbps
-> No need to patch
-> Not associated with security groups
-> Automatically assigned a public ip address.
-> This should be created for each availability zone
-> Remember to update Route Table.
-> No need to diable source/destination check
-> More secure than a NAT instance.

Network Access Control List
-> Your VPC automatically comes with a default network ACL, and by default it allows all outbound and inbound traffic.
-> You can create custom network ACLs. By default, each custom network ACL denies all inbound and outbound traffic until you add rules.
-> Each subnet in your VPC must be associated with a network ACL, the subnet is automatically associated with default network ACL.
-> You can associate a network ACL with multiple subnets; however, a subnet can be associated with only one network ACL at a time.When you associate a network ACL with a subnet, the previous association is removed.
-> Network ACL can contain numbered list of rules that is evaluated in order, starting with the lowest numbered rule
-> Network ACLs can have seperate inbound and outbound rules, each rule can either allow or deny traffic.
-> Network ACLs are stateless; thus responses to allowed inbound traffic are subject to the rules of outbound traffic (vice versa)
-> Ephermeral port (client to server communication port) is meant only for outbound rules.
-> To block any IP address, we can use only Network ACLs and not the Security Groups

When we create load balancer for an ec2 instance of an private VPC, following things should be considered 
-> Your subnet should have associated with internet gateway
-> You should choose atleast two subnets from different availability zone.
-> You can have only one subnet from one availability zone

VPC Flow Logs
-> VPC Flow logs is a feature that enables you to capture information about the IP traffic going to and from network interface in your VPC. 
-> Flow log data is stored using amazon cloudwatch logs.
-> After you have created a flow log, you can view and retrive its data in Amazon CloudWatch logs.
Flow logs can be created at three levels
-> VPC
-> Subnet
-> Network Interface Level
You cannot enable flow logs for VPC's that are peered with your VPC unless the peer VPC is in your account.
You cannot tag a flow log
After you have created a flow log, you cannot change its configuration; for example, you cannot associate a different different IAM role with the flow logs.
Not all the IP traffic is monitored.
-> Traffic generated by instances when they have contact the Amazon DNS server. If you use your own DNS server, then all the traffic to that DNS server is logged.
-> Traffic generated by Windows instance for Amazon WIndows license activation.
-> Traffic to and from 169.254.169.254 for instance metadata 
-> DCHP traffic
-> Traffic to reserved IP address for the default VPC router.

NAT Instance vs Bastion Instance
-> NAT instance is used to provide internet traffic to ec2 instance in private subnets.
-> A bastion is used to securely administer EC2 instances (using SSH or RDP) in private subnets. 

VPC Endpoints
-> Endpoint allows you to securely connect your VPC to another service. 
-> An interface endpoint is an elastic network interface(ENI) that serves as an entry point for traffic to the service.
-> A gateway endpoint serves as a target for a route in your route table for traffic destined to that service.
-> Allows certain public AWS service to securely connect to the private VPC

Network ACLs are stateless

Steps to create a basic interactive websites
Resources Involved:-
1. EC2
2. S3
3. Database
4. ELB and CloudFront

Step1: Create VPC and Secutity Groups as required.

WhitePaper Details:-
-> Almost zero upfront infrastructure investment
-> Just-in-time Infrastructure.
-> More efficient resource utilization
-> Usage-based costing
-> Reduced time to market

Technical Benefits of Cloud-
-> Automation - "Scriptable Infrastructure"
-> Auto Scaling
-> Pro-active scaling
-> More efficient Development lifecycle.
-> Improved Testability
-> Disaster Recovery and Business Continuity
-> "Overflow" the traffic of the cloud.

Design for Failure:

Rule of thumb: Be a pessimist when designing architectures in the cloud; assume things will fail. In other words, always design, inplement and deploy for automated recovery from failure.
In particular, assume that your hardware will fail. Assume that outages will occur. Assume that some disaster will strike your application. Assume that you will be slammed with more than the expected number of requests per second some day. Assume that with time your application software will fail too. By being a pessimist, you end up thinking about recovery strategies during design time, which helps in designing an overall system better.

Decouple your Components:-
The key is to build components that do not have tight dependencies on each other, so that if component were to die(fail), sleep(not respond) or remain busy(slow to respond) for some reason, the other components in the system are built so as to continue work as if no failure is happening.
In essence, loose coupling isolates the various layers and components of your application so that each component interacts asynchronously with the others and treats them as a "black box"

For example, in the case of web application architecture, you can isolate the app server from the web server and from the database. The app server does not know about your web server and vice-versa, this gives decoupling between layers and there are no dependencies code-wise or functional prespectives.
In case of batch processing architecture, you can create asynchronous components that are independent to each other.

The cloud brings three new concept of elasticity in your applications. Elasticity can be implemented in three ways:-

1. Proactive Cyclic Scaling
2. Proactive Event Based Scaling
3. Auto-scaling Scaling based on Demand.

Secure your application for different servers as mentioned below:-
1. Web server
2. App server
3. DB server
This is achieved using Security Groups and Network ACLs

Five pillars of well architecture framework:-
1. Security
2. Reliability
3. Performance Efficiency
4. Cost optimization
5. Operational Excellence

Structure of each pillars
-> Design Principles
-> Definition
-> Best Practices
-> Key AWS services
-> Resources

General Design Principles
-> Stop guessing your capacity needs
-> Test Systems at production scale
-> Automate to make architectural experimentation easier
-> Allow for evolutionary architectures
-> Data Driven Architectures
-> Improve through game days

Security
-> Design Level
Apply security at all layers
Enable traceabilities
Automate responses to your security events
Focus on securing your systems
Automate security best practices

Security in the cloud consist of 4 areas:-
-> Data protection
-> Previlage management
-> Infrastructure protection
-> Detective controls

Data Protection
Before you begin to architect security practices across your environment basic data classification should be in place.
You should organize and classify your data in to segments such as publicly available, available to only members in the organization, available only to board etc.
You should also implement a least previlage access systems so that people will be able to access only what they need.
However, most importantly, you should encrypt everywhere possible, whether it could be at rest or it transit

In AWS, the following practices help you to protect the data
-> AWS customers maintain full control over thier data.
-> AWS makes it easier for you to encrypt  your data and manage keys, including regular key rotation, which can be easily automated natively by AWS or maintained by a customer
-> Detailed loggibng is available that contains important content such as file access and changes.
-> AWS has designed storage systems for exceptional resiliency. As an example, Amazon S3 is designed for 11 nines of durability.
-> Versioning
-> AWS never initiates the movement of data between regions.

Data Protection Questions?
1. How are going to encrypt data in rest?
2. How are going to encrypt data in transit?

Previlage Management ensures that only authorized and authenticated users are able to access your resources, and only in the manner that is intented. It can include.
1. Access COntrol List(ACLs)
2. Role based access Controls.
3. Password Managements

Questions to be asked?
How are you protecting access to and use of AWS root account credentials?
How are defining roles and responsibilities of system users to conrol human access to the AWS Management Console and APIs?
How are limiting automated access?
How are you managing keys and credentials?

Infrastructure Protection is meant for VPC level.
Questions to be asked?
How are you enforcing network and host-level boundary protection?
How are you enforcing AWS service level protection.
How are you protecting integrity of the operating systems in the EC2 instances?

Detective Controls
This is used to detect security breaches. This can be achieved by below:-
1. AWS Cloud Trail
2. AWS CloudWatch
3. AWS config
4. Amazon Simple Storage Service
5. Amazon Glacier

1. How are you capturing and analyzing AWS logs?
Data Protection:
-> You can encrypt your data both in rest and transit using ELB,S3 and RDS.
Prvilage Managements
-> IAM, MFA
Infrastructure Protection
-> VPC
Detective Controls
-> AWS Cloud Trail, AWS Config, AWS Cloud Watch

Pillar Two- Reliability
The reliability pillar covers the ability of a system to recover from service or infrastructure outages/disruptions as well as the ability to dynamically acquire computing resources to meet demand.

Design Principles
-> Test recovery procedure
-> Automatically recover from failure
-> Scale horizontally to increase aggregate system availability
-> Stop guessing capacity

Reliability in cloud consist of three areas:-
1. Foundations
2. Change Management
3. Failure Management

Foundations
AWS handles most of the foundation for you unlike traditional data centers.
Cloud designed to be limited.
However there are certain service limits set by AWS which we need to be aware of.
Be aware of all the service limit AWS have and plan for additional service ahead.

Change Management
Most of the change management process are handled automatically in cloud with the help of CloudWatch and autoscaling.
Questions to be asked?
How does your system adapt to changes in demand?
How are you monitoring AWS resources?

Failure Managament
Questions to be asked?
How are you backing up the data?
How does your system withstand component failures?
How are you planning for recovery?

Key AWS resources
Foundations
->IAM, VPC
Change Management
-> AWS CloudTrail
Failure Management
-> CloudFormation

Pillar3- Performance Efficiency
The performance efficiency pillar focuses on how to use computing resources efficiently to meet your requirements and how to maintain that efficiency as demand changes and technology evolves.

Design Principles
-> Democratize advance technologies
-> Go Global in minutes
-> Use server-less architecture
-> Experiment more often

Performance Efficiency in cloud consist of 4 areas;
-> Compute
-> Storage
-> Database
-> Space-time trade-off

When architecturing your system, it is important to choose right kind of server.
Some applications may require heavy CPU utilization, some require heavy memory utilization
With AWS servers are virtualized and at the click of the button, you can change the type of the server in which your environment is running on. You can even switch to running with no servers at all and use AWS Lambda.
Questions to be asked:-
How do you select the appropriate instance type for your system?
How do you ensure that you continue to have the most appropriate instance type as new instance type as new instance types and feature are introduced?
How do you monitor your instances post launch to ensure they are performing as expected?
How do you ensure that the quatity of your instance matches your demand?

Storage:-
The optimal storage solution depends on number of factors
1. Access Method - Block, File or Object
2. Pattern of Access- Random or Sequencial
3. Throughput Required
4. Frequency of Access
5. Frequency of Update
6. Availability Contraints
7. Durability Constraints

Factors required for Database solutions
1. Database consistency
2. High availability
3. Do we need No-SQL

Space Time Off
You can use Read Replicas in RDS to spread out your load. This helps in lower latency.
You can use Direct Connect to provide predictable latency between HQ and AWS.
You can also use caching services such as Elastic Cache and CloudFront to reduce latency.
Questions to be asked?
How you select the appropriate proximity and caching solutions for your systems?
Key AWS service
Compute
->Lambda, Autoscaling
Storage
-> EBS, S3, Glacier
Database
-> RDS, DynamoDB, Redshift
Space-Time Trade-Off
-> CloudFront, Direct Connect, Read Replicas, Elastic Cache?

Pillar Four - Cost Optimization
This helps in reducing your costs to a minimum and use those savings for other parts of the systems.
-> Transparently Attribute expenditure.
-> Use managed service to reduce cost of ownership
-> Trade capital expense for operating expense
-> Benefit from ecnomies of scale
-> Stop spending money on data center operations.

Definition
-> Match supply and demands
Autoscaling
-> Cost-effective resources
EC2
-> Expenditure awareness 
CloudWatch Alarams, SNS
-> Optimizing over time
AWS blog, AWS Trust

Match and Supply
-> Doesnt need to buy datacenter in upfront. In cloud, we can scale-up and scale-down based on the needs.
-> With cloud you no longer have to go out and get quotes on physical servers,choose a supplier, have those resource delivered.
Question to be asked?
How do you manage and/or consider the adoption of new services?

Operational Excellance Pillar=> Pillar 5
-> This includes operational practices and procedures used to manage production workloads.
-> This include on how planned changes are executed as well as responses to the unconditional operational events
-> Change execution and responses should be automated. All processes and procedures of operartional excellence should be documented, tested and regularly reviewed.

Design Principles
-> Perform operations with code.
-> Align operations processes to business objectives
-> Make regular,small, incremental
-> Test for responses to unexpected events.
-> Learn from operational events and failures
-> Keep operations procedure current

There are three best practice areas for Operational Excellence in the cloud
->Preparation
->Operation
->Response

Prepartion
-> Effective prepartion is required to drive operational excellance
-> Operational checklist always ensures that workloads are ready for production operation, and prevent unintentional production promotion without effective preparation.
-> Workloads should have
  a. Runbooks - operations guidance that operations teams can refer to as to perform normal daily tasks.
  b. Playbooks- Guidance for responding to unexpected operational response.
  
 Cloud Formation can be used to ensure that environments contain all required resources when deployed in production, and that the configuration of the environment is based on the tested best practices, when reduces the oppurtunities for human errors.
 Autoscaling->response to events when business related events affect operational needs.
 AWS Config-> AWS config rule feature create mechanism to automatically track and respond to changes in your AWS workloads and environments.
 Tagging-> helps in identifying resources when needed during operations and responses.
 
 Questions to be asked?
 What are the best practices for cloud operations are you using?
 How are you doing configuration management for your workload?
 
 Documents should contain details as mentioned below:
 1. Application designs
 2. Environment Configurations
 3. Resource Configurations
 4. Response Plans
 5. Mitagation Plans
 
 Operations
 -> Operations should be standaridized and manageable on a routine basis.
 
 Questions to be asked?
 How are you evolving your workload while minimizing the impact of change?
 How do you monitor your workload to ensure it is operating as expected?
 
 Deployment, Release Management, changes and Rollbacks should be avoided.
 Release should be small and frequent and no downtime should be there.
 Monitoring should be aligned with business needs.
 
 Responses to unexpected events should be automated
 This is not just for alerting but also for mitigation, remediation, rollback and recovery.
 
Questions to be asked?
How do you respond to unplanned operational events?
How is escalation managed when responding to unplanned events?

Preparation
SQS, Autoscaling, AWS Config
Operations
CodeCommit, Code deploy and AWS Code Pipeline
Responses
Cloud Watch, SNS
