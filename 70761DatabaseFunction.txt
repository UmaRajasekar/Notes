There are different database functions available as listed below:-
1. Aggregate function:-
These functions perform calculations on set of values and return a single value. They are allowed in HAVING clause with the combination of GROUP BY clause of a select statement
You can also use aggregate function in OVER clause, to aggregate on only certain categories of ROW
The select list of SELECT statments

Different Aggregate function
APPROX_COUNT_DISTINCT- evaluates an expression for each row in a group and returns approximate value of unique not null values in a group
AVG
MAX
MIN
SUM
CHECKSUM_AGG - Returns checksum of the group. It ignores NULL values. It can be applied for ALL values and DISTINCT values. This helps in   tracking changes to the field.
COUNT
COUNT_BIG - operates like a COUNT function, only difference is that it returns BigINT instead of INT
GROUPING - specifies whether the specified column expression in a GROUP BY list is aggregated or not. It is mainly used to distinguish        from NULL values returned by ROLLUP,CUBE or GROUPING SETS
GROUPINGID- It computes the level of grouping. It can be used in SELECT, ORDER BY, HAVING clause. The columns in the GROUPING ID should      match exactly the columns in the GROUP BY clause. GROUPING_ID interprets that string as a base-2 number and returns the equivalent        integer.
STDEV - Returns standard derivation of all the values. Ignores NULL values
STDDEVP - Returns standard derivation for population of all values in the specified expression. It ignores NULL values.
VAR - Retruns variance of all the values. Ignores NULL values.
VARP - Returns variance for the population of all the values. Ignores NULL values.

2. Analytic Functions
Analytic Function calculated an aggregate value based on a group of rows. Unlike aggregate function, analytic function returns multiple rows for each group.Use analytic function to compute moving averages, running totals, percentage or top-N results within a group.

Different Analytic Function
CUME_DIST- This function calculated cummualtive distribution of values within a group of values. This calculates the relavtive position    of a specified value in a group of values. The value will be greater that 0 and less than or equal to 1.
FIRST_VALUE - Returns first value in an ordered set of values.
LAST_VALUE - Returns last value in an ordered set of values.
LAG - Returns record from previous row of the result set
LEAD - Returns record from next row of the result set
PERCENTILE_COUNT- Calculates percentile based on a continous distribution of the column value in SQL server
PERCENTILE_DISC - Calcilates percentile for sorted value in a distint rowset or within distinct partition in SQL Server
Notes:- The difference between PERCENTILE_COUNT and PERCENTILE_DISC is that PERCENTILE_COUNT result is not necessary to be equal to specific value in the column. PERCENTILE_COUNT(0.5) will return MEDIAN value of the table.
PERCENTILE_RANK - Calculates the relative rank of the row within a group of rows. This is similar to CUME_DIST, only difference is that CUME_DIST calculates distribution and PERCENTILE RANK calculates rank of that distribution.

3. Collation Function

Different Collation Funtion
Collation Property
COLLATIONPROPERTY( collation_name , property )  -> This function returns property of the specified collation. 'property' can be CodePage, LCID, ComparisionStyle, Version

TERTIARY_WEIGHTS
TERTIARY_WEIGHTS( non_Unicode_character_string_expression ) -> Returns binary string of weights for each character in a non-unicode string expression.

Configuration
These are scalar functions return information about current configuration option settings

Different configuration function
@@DBTS -> Returns currunt timestamp.
@@LANGID -> Returns local language ID of the language that is currently being used. This can also be using variable 'SET LANGUAGE 'Language''
@@LANGUAGE-> Returns language curruntly being used. You can also set language.
@@LOCK_TIMEOUT -> Returns current lock timout settings in milliseconds for the current session. You can also set your own lockout time. This allows an application to set the maximum time that a statement waits on a blocked resource. returns in milliseconds.
@@MAX_CONNECTIONS -> Returns maximum number of user connections allowed  on an SQL server instance.
@@MAX_PRECISION -> Returns precision level used by decimal and number data types 
@@NESTLEVEL -> Returns nesting level of current stored procedure execution(initially 0) on the local server.
@@OPTIONS -> Returns information of current SET operation.
https://docs.microsoft.com/en-us/sql/database-engine/configure-windows/configure-the-user-options-server-configuration-option?view=sql-server-2017
@@OPTIONS returns number in the form of addition of 2 power number as mentioned in the above web page. This helps us to figure on which option is activated.
@@REMSERVER - This enables stored procedure to check the name of the database server from which procedure is running.
@@SERVERNAME - Returns name of local server that is running SQL server
@@SERVICENAME - Returns service name of registry key under which SQL server is running. For default server, it returns 'MSSQLSERVER'.
@@SPID - Returns session ID of the current user.
@@TEXTSIZE - Returns current value of text size. You can also set text size
@@VERSION - Returns system and build information  for the current installation of SQL Server

Conversion
This function helps in datatype casting and conversion

Different conversion functions are listed below:-
a. CAST and CONVERT
This helps in converting one datatype to another. Casting Decimal number as int. Convert, converts datatype into desired datatype
-- CAST Syntax:  
CAST ( expression AS data_type [ ( length ) ] )  
-- CONVERT Syntax:  
CONVERT ( data_type [ ( length ) ] , expression [ , style ] )
The differnce between int and numeric is that int datatype truncates the number and numeric datatype rounds off the number
In convert datatype, char converts binary datatype to character datatype
In convert datatype, binary converts char datatype to binary datatype.
You can cast datetime as date, time and datetime
b. PARSE
Parse is mostly used to convert string data type into number and date/time datatype.
c. TRY_CAST
Retruns a value cast to the specified data type if the casts suceeds; otherwise returns NULL
d. TRY_CONVERT 
Returns a value cast to the specified data type if the casts succeeds; otherwise returns NULL
Whenever CAST or CONVERT is not explicitly permitted, then SQL server throws an error.
Ex:- type cast converting of int datatype to xml datatype is not allowed.
e. TRY_PARSE
Returns the result of expression, translated to a requested data type, or null if casts fails in SQL server.

Crytographic Function
These functions support digital signing, digital signature validation, encryption, and decryption
a. ASYMKEY_ID
SQL server provides options to create aysmetric key. ASYMKEY_ID returns key ID of that particular asymmetric key.
b. ASYMKEYPROPERTY
By taking ASYMKEY_ID as input , it returns algorithm description, SID of an asymmetric key in nvarchar() and binary format
c. CERT_ID
SQL server provides options to create certificate with the help of password or private key. CERT_ID returns certificate ID of that certificate
d. CERTPROPERTY
By taking certID, it can return property as Expiry_Date, Start_Date, Issuer_Name, Cert_Serial_Number, Subject, SID, String_SID
e. CRYPT_GEN_RANDOM
This function returns a random cryptographic, randomly-generated number, generated by Crypto API. Returns a hexadecimal number with a length of specified number byte.
f. EncryptByAsymKey  and DecryptByAsymKey
Above function encrypts a plain text and decrypts the same with the help of asymetric key.
g. DECRYPTBYCERT and ENCRYPTBYCERT
Above function encrypts a plain text and decrypts the same with the help of certificate.
h. ENCRYPTBYKEY and DECRYPTBYKEY
Above function encrypts a plain text and decrypts the same with the help of symmetric key.

CURSOR
@@CURSOR_ROWS- This returns the number of qualifying rows currently in the last cursor opened on the connection.
  Returns negative value if the cursor set is opened asynchronously
  Returns -1 if the cursor is dynamic.
  0 No cursor is opened
  n Returns total number rows in the cursor
@@FETCH_STATUS - Issues status of the last FETCH command issued.
  0 - The fetch statement was successful.
  -1 - The fetch statement was failed or row was beyond the result set.
  -2 - The row fetched is missing
  -9 - The cursor is not performing a fetch operation
CUSROR_STATUS - For given parameters, CURSOR_STATUS shows whether or not a cursor declaration has returned a cursor and result set.
  1 - Cursor returns atleast one row
  0 - The cursor result set is empty
  -1 - The cursor is closed
  -2 - Not applicable
  
Logical Functions:-
1. CHOOSE(array_index,value1,value2,value3)
2. IIF(boolean,true, false)

Ranking Function:-
Ranking function returns a ranking value for each row in the partition. Depending on the function that is used, some rows might recieve the same value as other rows.
1. DENSE_RANK- Returns rank with no gaps in the ranking.
2. NTILE- Distirbutes the rows in an ordered partition into a specified number of groups. If group of 50 statements needs to grouped in 8 grpups, then first 2 groups will have 7 rows and remaining groups will have 6 rows
3. RANK - Returns rank of each row within partition of result set. for example 1, 2, 2, 4, 5
4. ROW_NUMBER- Returns sequencial number and does not have duplicate rank

String Functions
1. ASCII - returns ASCII value of the left most character
2. CHAR - Returns character of the correspinding ASCII value. For any value other than 0-255, it returns NULL.
3. CHARINDEX - Returns starting position of the first argument expression available in second argument expression.
4. CONCAT - Concatenation of strings
5. CONCAT_WS - Concatenates the string with the help of seperator. Both CONCAT and CONCAT_WS ignores NULL
6. SOUNDEX - Returns four character code to evaluate the similarity of two things.
7. DIFFERENCE - Returns integer value measuring the difference between two SOUNDEX function of the string. DIFFERENCE function returns 4    if there are less difference in SOUNDEX function of two string. If there are larger difference, then returns 0.
8. FORMAT - Mostly used to return date/time in desired format. You can also format number as numeric, general and currency format.
9. LEFT - Returns left part of a character string with specified number of characters
10. LEN - Returns length of the string
11. LOWER - Retruns lower case of the string
12. LTRIM - Trims the whitespace of the string
13. UNICODE - Same as ASCII, but returns value in UNICODE standard
14. NCHAR - Same as CHAR (which returns character of ASCII value), instead it returns character UNICODE value
15. PATINDEX - Pattern Index. Returns starting position of first occurence of the pattern in a given string.
16. QUOTENAME - Returns the string in quoted character.
17. Replace - Replaces all the occurence of the string characters
18. Replicate - Replicates the specified string character.
19. STR - Converts character data coverted from numeric
20. STRING_AGG - Concatentes the value of string expression and places seperator values between them.
21. STRING_ESCAPE - Escapes special character in text and returns text with escaped character
22. STRING_SPLIT - Opposite to String aggregate. This splits string into various rows based on the seperator.
23. STUFF - This function inserts a string into another string. It deletes the specified length of the characters in the first string at the start position and then inserts the second string into the first string at the start position.
24. Returns part of the string.

Datatype function
1. DATALENGTH - Returns number of bytes used to represent any expression.
2. IDENT_CURRENT - Returns last identity generated for the table. 
    @@IDENTITY- returns last identity generated for any  table
    SCOPE_IDENTITY - returns last identity generated for any table through insert statements
3. IDENT_INCR - returns increment value of the increment column which is set during creation
4. IDENT_SEED - sreturns seed value of the increment column which is set during creation
5. IDENTITY - This function can be only used while creating a new table using SELECT statements
6. SQL_VARIANT_PROPERTY - SQL variant is the datatype which helps to store all kinds of data type supported by SQL server. It returns property of the column such as BaseType, Precision, Scale, Collation, TotalBytes and maxklength

Date and Time function
@@DATEFIRST - Returns first day which is set in database(by default, it is SUNDAY for us_english. You can also set your own day)
CURRENT_TIMESTAMP - Retruns current database timestamp. Same as GETDATE()
SYSDATETIME - Returns system date and time.
SYSDATETIMEOFFSET - Returns system date and time and also indicates offset from universal timezone
SYSUTCTIME() - Returns universal time zone. Same as GETUTUCDATE()
DATEADD - Helps to add datepart. This can be YEAR, MINUTE, HOUR, DAYOFYEAR, DAY,week minutes, seconds, milliseconds and nanoseconds
DATEDIFF - Helps in getting datedifference of any of the parts YEAR, MINUTE, HOUR, DAYOFYEAR, DAY,week minutes, seconds, milliseconds and nanoseconds
DATEDIFF_BIG - Same as DATEDIFF, only difference is that it returns as Big int.
DATEFROMPARTS - Returns date from year, month and day part.
DATENAME - Returns date part specified in argument as character
DATEPART - Returns date part specified in argument as integer.
DATETIME2FROMPARTS - Has argument of day, month, year, hours, minutes, seconds, fraction and precision and returns datetime2.
DATETIMEFROMPARTS - Has argument of day, month, year, hours, minutes, seconds and milliseconds and returns datetime
DATETIMEOFFSETFROMPARTS - Has argument of day, month, year, hours, minutes, seconds, fraction, hour_offset, minute_offset, precision)
DAY - Same as datepart(day,date)
YEAR - Same as datepart(year, date)
MONTH - Same as datepart(month, date)
EOMONTH - Returns last day of the month.
ISDATE - Returns 1 if its valid date.
SMALLDATETIMEFROMPARTS - Has argument of day, month, year, hours and minutes
SWITCHOFFSET - This is used to switch current time(in the format of datetimeoffset) to different timezone.
TIMEFROMPARTS - Has argument of hours, minutes, seconds, fraction and precision and returns time
TODATETIMEOFFSET - Helps in changing time to different timezone

Joins:-
Nested Loop Join:-
  A nested loops join is particularly effective if the outer input is small and the inner input is preindexed and large.
Merge Joins:-
  If both join inputs are large and the two inputs are of similar sizes, a merge join with prior sorting and a hash join offer similar performance. Join on indexed column on equal number of rows of table.
Hash Joins:-
  Hash joins can efficiently process large, unsorted, nonindexed inputs. They are useful for intemediate results in complex queries.
There are two phases.
  a. Build Phase:-
    It reads rows of smaller table into the memory. It hashes the keys of the rows which will be used for join.
  b. Probe Phase:-
    It reads rows of other table and hashes the keys of the rows which will be used for join. While doing this it checks for the matching rows on hashed keys in the table build in Build phase. Smaller table in memory and larger table in disk is basic rule. If smaller table does not fit in memory it spits to hard drive. DBSPACETEMP configuration parameter is used to stored hashed table in probe phase.
There are three types of hash joins
  1. In-memory hash join.
  2. Grace hash join.
  3. Recursive hash join.

Common Table Expression:-
Specified as temporary named result
It can be used in SELECT, INSERT, UPDATE and DELETE statement and can also be used in CREATE VIEW statements.
Guidelines for using non-recursive CTE:-
A CTE must be followed by SELECT, INSERT, UPDATE and DELETE statement and can also be followed in CREATE VIEW statements.
Multiple CTE query definitions can be defined in a non-recursive CTE. The definition must be combined by one of these set operators: UNION, UNION ALL, EXCEPT and INTERCEPT
CTE can reference itself and previously defined CTEs in the same WITH clause. Forward referencing is not allowed.
Specifying more than one WITH clause is not allowed. Ex:- Subquery cannot have another WITH clause defined with a CTE.
There are few clauses cannot be used in the CTE query definition. ORDER BY, INTO, OPTION, FOR BROWSE
When a CTE 
Guidelines for recursive CTE's:-
A recursive CTE's consist of two query definition. One is anchor member query definition and another is recursive member. You can have multiple anchor member and multiple recursive member. You must define all the anchor member before recursive member.
All the query member of anchor unless they refer themselves.
Anchor members must be combined by one of these set operators: UNION ALL, UNION, INTERSECTION and EXCEPT
The number of columns in the anchor and recursive must be same.
The datatype of the columns in the anchor and recursive member should also be same.
The FROM clause of recursive member should refer only one time to CTE expression name.
The items such as SELECT DISTINCT, GROUP BY, HAVING, Scalar aggragation, TOP, LEFT OUTER JOIN, RIGHT OUTER JOIN, PIVOT and sub-queries are not allowed in recursive member.

Sub-query Rules:-
1. The select list of a subquery introduced with a comparision operator can include only one expression or column name(except that EXISTS and IN operate on "SELECT *" or a list respectively)
2. If the WHERE clause of an outer query includes a column name, it must be a join-compatible with the column in the subquery select list)
3. The ntext, text, image datatypes cannot be used in the select list of queries.
4. Because they must return a single value, subqueries introduced by an unmodified comparision operator(one not followed by the keyword ANY or ALL) cannot include GROUP BY and HAVING clause.
5. The distinct cannot be used in subquery that include GROUP BY.
6. ORDER BY can be specified only when TOP is specified.
7. The COMPUTE and INTO clause cannot be specified.

Control-Of-Flow
1. BEGIN .. END -> encloses a series of T-SQL statements so that a group of T-SQL statements can be executed. 
2. BREAK - exits current while loop. If current while loop is nested with another, then it exits only one WHILE LOOP.
3. CONTINUE - Restarts WHILE loop. Any statements after the CONTINUE keyword are ignored. 
4. IF.. ELSE - Imposes condition on the execution of T-SQL expression. This can include sql_statement or statement_block.
5. GOTO- Alters the flow of execution
6. RETURN - Exits unconditionally from a query or procedure. RETURN is immediate and complete and can be used at any point to exit from a procedure, batch, or statement block. Statement that follow RETURN are not executed.
7. THROW - Raises exception and transfers execution to a catch block of a TRY-CATCH construct.
If a try-catch construct is not available, the statement batch is terminated. The line number and procedure where the exception is raised are set. The severity is set to 16.
If the throw statement is specified without parameters, it must appear inside the CATCH block. This causes caught exception to be raised. Any error that occurs in a THROW statement causes the batch to be terminated
% is the reserved character.
Difference between RAISEERROR and THROW
1. msg_id passed to RAISE error should be available in sys.messages
2. msg_str can contain printf formatting styles
3. The severity parameter specifies the serverity of the exception.
TRY-CATCH - Every try block should immediately follow with Catch block. A try-catch cannot span multiple branches. You can retrive error infomation using ERROR_NUMBER(), ERROR_SEVERITY(), ERROR_STATE(), ERROR_PROCEDURE(), ERROR_LINE(), ERROR_MESSAGE()
Try catch do not trap following conditions:- 
1.Warning and informal messages with severity less than 10.
2.Severity greater than 20
3.Client-interrupt request
4.Session ended by sys admin
Following types of error not handeled in the CATCH statements
1. Compilation errors.
2. Errors that occur during statement-level recompilation
3. Object name resolution errors.
XACT_STATE return -1 if the transaction is uncommittable.
SET XACT_ABORT ON; set the transaction as uncommittable when foriegn key violation happens
WAIT FOR - Blocks the execution of a batch, stored procedure, or transaction until a specified time(TIME) or time interval(DELAY) is reached, or a specified statement modifies or returns at least one row
WHILE - Sets a condition for a repeated execution of an SQL statement or statement block. The statements are executed repeatedly as long as the specified condition is true. The execution of statements in the WHILE loop can be controlled from inside the loop with the BREAK and CONTINUE keywords.

Transaction Statements
1. A Transaction is a single unit of work. If a transaction is sucessful, all of the data modifications made during the transactions are committed and become permanenet part of a database. 
2. If a transaction encounters errors and must be cancelled or rolled back, then all of the data modifications are erased.
SQL Server operates in the following transaction nodes:
1. Autocommit transactions - Each individual statement is a transactions
2. Explicit transactions - Each transaction is explicitly started with the BEGIN TRANSACTION statement and explicitly ended with a COMMIT or ROLLBACK transactions  
3. Implicit transactions - A new transaction is implicitly started when the prior transaction completes, but each transaction is explicitly completed with a COMMIT or ROLLBACK statements.
4. Batch-Scoped transactions - Applicable only to multiple active result sets (MARS), a Transact-SQL explicit pr implicit transaction that starts under a MARS session becomes a batch- scoped transaction. A batch-scoped transaction that is not committed or rolled back when a batch completes is automatically rolled back by SQL server.
Different Transaction Statements:-
1. BEGIN DISTRIBUTED TRANSACTION 
  For example, if BEGIN DISTRIBUTED TRANSACTION is issued on ServerA, the session calls a stored procedure on ServerB and another stored procedure on ServerC. The stored procedure on ServerC executes a distributed query against ServerD, and then all four computers are involved in the distributed transaction. The instance of the Database Engine on ServerA is the originating controlling instance for the transaction.
  The session involved in T-SQL distributed transactions do not get a transaction object they can pass to another session for it to explicitly enlist in the distributed transaction.
  The only way for a remote server to enlist in the transaction is to be the target of a distributed query or a remote stored procedure call.
  The SET option REMOTE_PROC_TRANSACTIONS ON will help to elavate BEGIN transaction into BEGIN DISTRIBUTED transaction when there is a distributed query or remote server call available inside BEGIN TRANSACTION
BEGIN TRANSACTION
1. Providing Transaction name is mandatory when WITH MARK option is available. WITH MARK allows for restoring a transaction log to a named mark.
2. When SET IMPLICIT_TRANSACTIONS is set to ON, a BEGIN TRANSACTION statement creates two nested transactions.
3. Additionally, transaction log marks are necessary if you need to recover a set of related databases to a logically consistent state. Marks can be placed in the transaction logs of the related databases by a distributed transaction
COMMIT TRANSACTION
1. Marks the end of a successful implicit or explicit transaction.
2. If @@TRANCOUNT is 1, COMMIT TRANSACTION makes all data modifications performed since the start of the transaction a permanent part of the database, frees the resources held by the transaction, and decrements @@TRANCOUNT to 0. If @@TRANCOUNT is greater than 1, COMMIT TRANSACTION decrements @@TRANCOUNT only by 1 and the transaction stays active.
3. When used in nested transactions, commits of the inner transactions do not free resources or make thier modifications permanenet. The data modifications are made permanent and resources freed only when the outer transaction is committed.
4. Only one WITH MARK option is applicable in the nested transactions and if multiple MARK statements are available then warning is thrown (not error)
5. In nested transaction, the commit or rollback details of nested transaction will not be committed, but will be committed or rolled back (mentioned in the inner block) post the commit or rollback of outer transaction.
SAVEPOINT
A user can set a savepoint or marker within a transaction
The savepoint defines the location to which a transaction can return if part of the transaction is conditionally cancelled.
If a transaction is rolled back to a savepoint, it must proceed to completion with more Transact-SQL statements if needed and a COMMIT TRANSACTION statement, or it must be canceled altogether by rolling the transaction back to its beginning.
To cancel an entire transaction, use the form ROLLBACK TRANSACTION transaction_name. 
Duplicate savepoint names are allowed in a transaction, but a ROLLBACK TRANSACTION statement that specifies the savepoint name will only roll the transaction back to the most recent SAVE TRANSACTION using the name.
SAVE Transaction is not supported in the distributed transactions

Create and Query database objects:-
Transact-SQL statements can be written and submitted to the database engine in the following ways:-
1. By using SQL Server Management Studio
2. By using sqlcmd utility
3. By connecting from the application you create
a. create database sql statements
create database database_name;
To create a new login
pre-requisite:- create a windows login
1. Create a database login. [create login computer_name\Mary from WINDOWS]
2. To create a user in database [create user Mary for LOGIN [computer_name\Mary]]
Grant and Revoke permission
GRANT EXECUTE ON pr_Names FROM Mary;
REVOKE EXECUTE ON pr_Names FROM Mary;

.WRITE Option in the update statement
Syntax : column.write(expression, @offset, @length)
@offset - Null -> appends text at the end
@length - Null -> Removes all the existing text

OUTPUT option:-
Displays all the data impacted after DML statements such as INSERT, DELETE and UPDATE
<OUTPUT> <DML SELECT STATEMENT ([INSERTED|DELETED].[*|columnname])> <INTO> <tablename|tablevariable 
Remarks
1. The output of the computed column in the OUTPUT clause is not computed.
2. No gurantee in the order of the changes applied
3. OUTPUT Clause will not work on following scenarios
  a. DML statements that reference local partitioned views, distributed partitioned views, or remote tables.
  b. INSERT statements that contain an EXECUTE statement.
  c. Full-text predicates are not allowed in the OUTPUT clause when the database compatibility level is set to 100.
  d. The OUTPUT INTO clause cannot be used to insert into a view, or rowset function.
  e. A user-defined function cannot be created if it contains an OUTPUT INTO clause that has a table as its target.
To prevent nondeterministic behavior, the OUTPUT clause cannot contain the following references:
  a. A column from a view or inline table-valued function when that column is defined by one of the following methods:
    a. A sub-query
    b. A user-defined function that performs user or system data access, or is assumed to perform such access.
    c. A computed column that contains a user-defined function that performs user or system data access in its definition.

Triggers
1. DDL triggers fire in response to a variety of DDL events. These events primarily corresponds to T-SQL statements that start with keywords CREATE, ALTER, DROP, GRANT, DENY, REVOKE or UPDATE Statistics.
Use DDL statements to perform following 
  a. Prevent certain changes to your database schema.
  b. Having something occur in the database in response to a change in your database schema.
  c. Record changes or events in the database schema.
Types of DDL triggers
  a. T-SQL DDL Trigger
  Trigger in response to server-scoped or database-scoped event. statement executed ALTER SERVER CONFIGURATION or DROP TABLE statements executed.
  b. CLR DDL Trigger
  DDL triggers fire only after the DDL statements that trigger them are run. 
  You cannot use INSTEAD OF statements
  DDL trigger do not fire in response to events that affect local or temporary tables and stored procedures
  The information about an event that fires a DDL trigger, and the subsequent changes caused by the trigger, is captured by the EVENTDATA function.
  DDL triggers are not scoped to schemas. Therefore, functions such as OBJECT_ID, OBJECT_NAME,OBJECTPROPERTY and OBJECTPROPERTYEX cannot be used to querying metadata about DDL triggers. Use catalog views instead.
  create DDL and DML trigger comes with the option such as ENCRYPTION and EXECUTE AS CLAUSE
  Create DML trigger on memory optimized table comes with the option such as SCHEMABINDING and NATIVE_COMPILATION
  DDL Trigger has option as FOR and AFTER (CREATE,ALTER, DROP, GRANT, DENY ) wheraes DML statement has option for FOR, INSTEAD OF and AFTER 
EVENTDATA function
Returns an xml value contains time of the event, System process ID, type of event that fired the event
You can also use inserted or deleted tables in trigger to perform some validation, Test for errors and take action based on the error.
Find the difference between the state of the table before and after a data modification and take actions based on that difference.

Mathematical Functions
ABS - returns positive value. Doesnt have any imapact on float values.
ACOS - Function returns angle in radians. Only value from -1 and 1 is valid. For other it returns NULL where ASIN returns DOMAIN error.
Other trignometric expression - ASIN, ATAN, SIN, COS, TAN , COT
ATN2 - Returns the angle in radians between the positive X-axis and the ray of origin to a point 
CIELING- Returns smallest integer greater than or equal to, the specified numerical expression.
DEGREES - Returns degree for the radians passed. Radians will be provided as input to trignometric fuctions.
LOG - returns natural algorithm. The default base is 2.718281828.
POWER - returns the value of specified expression to the specified power.
RADIANS- returns radians for degree entered.
FLOOR - Returns largest integer less than or equal to specfied numberic expression
ROUND - For rounding off.
RAND - assigns random number. Repetitive calls of RAND() with the same seed value return the same results. Returns random float value from 0 to 1.
SIGN - Returns positive(+1), negative(-1) or zero of the specified signed integer
SQRT, SQUARE - Returns square root and square of the number respectively.

Trigger Functions:-
Column Updated-
  1. This function returns a varbinary bit pattern indicating the inserted or updated columns of a table or view
  2. COLUMNS_UPDATED tests for UPDATE or INSERT actions performed on multiple columns. To test for UPDATE or INSERT attempts on one column, use UPDATE().
  3. COLUMNS_UPDATED returns one or more bytes that are ordered from left to right. The rightmost bit of each byte is the least significant bit. The rightmost bit of the leftmost byte represents the first table column in the table, the next bit to the left represents the second column, and so on. 
  4. COLUMN_UPDATED returns multiple byte. One byte is equal to 8 bit. 2^0 (right most of the left most bit is column-1)
  5. You need to use substring function to validate if your column number in the table is more than 8.
TRIGGER_NESTLEVEL
  Returns number of triggers executed for the satement that fired the trigger. This is used to both DML and DDL trigger to find out number of nesting.
UPDATE 
  Returns a Boolean value that indicates whether an INSERT or UPDATE attempt was made on a specified column of a table or view
  UPDATE() is used anywhere inside the body of a Transact-SQL INSERT or UPDATE trigger to test whether the trigger should execute certain actions.

JSON function:
  1. ISJSON -> used to detect whether specified string or column data is in valid JSON format
  2. JSON_VALUE -> used to extract value by providing path of an JSON expression. JSON_VALUE(expression, path). extracts as an object or array. If unable to return object then, error is thrown stating using JSON_QUERY to retrive as Scalar function,
  3. JSON_QUERY -> extracts as scalar function instead of object or array.
  4. JSON_MODIFY -> used to update value of JSON file. Various opreration such as INSERT, APPEND, UPDATE and DELETE JSON value. Need to check if it works for SCALAR function
  
ROWSET function:
  1. OPENDATASOURCE(provider name, init string)
  provider name- Is the name registered as the PROGID of the OLE DB provider used to access the data source. 
  2. OPENQUERY(linked server, query)
  3. OPENROWSET -used in a FROM clause
  4. OPENJSON and OPENXML - Returns JSON and XML data in the tabular format
Retrieving data in JSON is available in 2 modes. LAX and STRICT mode. In strict mode, if syntax is incorrect, it returns error but in lax mode, it returns NULL

Replication function:
  PUBLISHINGSERVERNAME- Returns the name of the originating Publisher for a published database participating in a database mirroring session
  
System:
  $PARTITION - $PARTITION.<<Partition name>> (value). Returns the partition number in which a set of partitioning column values would be mapped for any specified partition funtion in SQL server 2017
  @@ERROR - Returns an error number when previous statement encountered an error.
  @@IDENTITY
  @@PACK_RECEIVED - Returns the number of input packets read from the network by SQL server since it was last started.
  @@ROWCOUNT - Returns number of rows affected by last statement
  @@TRANCOUNT - Returns number of begin transaction statement.
  BINARY_CHECKSUM - Returns the binary checksum computed over a row of a table or over a list of expression.BINARY_CHECKSUM and CHECKSUM are similar functions: They can be used to compute a checksum value on a list of expressions, and the order of expressions affects the resultant value. 
  COMPRESS- Compresses the expression
  CONNECTIONPROPERTY - Returns connection property
  CURRENT_REQUEST_ID
  CURRENT_TRANSACTION_ID
  ERROR_LINE
  ERROR_MESSAGE
  ERROR_NUMBER
  ERROR_PROCEDURE
  ERROR_SEVERITY
  ERROR_STATE
  FORMATMESSAGE- Formats the message, before it print. Concats string in place of %s, %i - positive int, %d- negative int
  GET_FILESTREAM_TRANSACTION_CONTEXT - Returns a token that represents the current transaction context of a session. 
  GETANSINULL - Returns the default nullability for the database for this session.
  HOST_ID() - Returns workstation ID
  HOST_NAME()
  ISNULL
  ISNUMERIC
  MIN_ACTIVE_ROWVERSION - returns rowversion values by using MIN_ACTIVE_ROWVERSION.
  NEWID
  NEWSQUENTIALID
  XACT_STATE - Returns user transaction state

CROSS OR OUTER APPLY is similar to the CROSS JOIN or OUTER JOIN that helps to retrieve matching data from table valued function
The value returned by JSON can be int, NULL, STring, array, object , true/false

COALESCE - Evaluates the arguments in order and returns the current value of the first expression that initially does not evaluate to NULL
NULLIF- Return NULL if both the columns are same.
CASE - The CASE expression has two formats:
  1. The simple CASE expression compares an expression to a set of simple expressions to determine the result.
  2.The searched CASE expression evaluates a set of Boolean expressions to determine the result.
  
Different FOR XML type
1. RAW - Generates one xml for one row
2. AUTO - Suitable to create nested xml. No control over the structure as it is autogenerated
3. EXPLICIT - <<ElementName!TagNumber!AttributeName!Directive>>

CREATE PROCEDURE
  -> Creates a Transact SQL or common language runtime (CLR) stored procedure in SQL server. 
  -> Accepts input parameters and returns multiple values as output with the help of output parameters
  -> Contains programming statements that perform operations in the database, including calling other procedures.
  -> Returns a status value to the calling procedure or batch to indicate success or failure.
  Syntax: create or alter procedure procedure_name {@parameter data_type} [VARYING] [=default] [OUT| OUTPUT | READONLY]
  [WITH <<PROCEDURE OPTION>>] [FOR REPLICATION] AS BEGIN <<Sql statements>> END;
  Procedure options can be encryption, recompile and execute as clause.
  VARYING - output parameters gets changed and it is valid for cursor parameters.
  READONLY - indicates parameter cannot be updated once passed.
  DEFAULT - By specifying default, procedure can execute parameter without being passed.
  RECOMPILE - Does not cache query plan. Forces to compile each time it executed.
  Parameters cannot be declared if FOR replication is specified.
  cursor data types can only be OUTPUT parameters and must be accompanied by the VARYING keyword
  Recompile option cannot be used when there is FOR Replication option available.
  Procedure options are execute as clause, Recompile and Encryption
  
CREATE VIEW
  CREATE OR ALTER VIEW <view name> [(column_name)]
  [WITH <view attribute>]
  AS SELECT Statment
  [WITH CHECK OPTION]
  View attribute can be encryption, schemabing and view_metadata
  Schemabinding - Binds the view to the schema of the underlying table or tables. When SCHEMABINDING is specified, the base table or tables cannot be modified in a way that would affect the view definition. The view definition itself must first be modified or dropped to remove dependencies on the table that is to be modified.
  Specifies that the instance of SQL Server will return to the DB-Library, ODBC, and OLE DB APIs the metadata information about the view, instead of the base table or tables, when browse-mode metadata is being requested for a query that references the view
  
  If the data is modlified with the views, it make sures that data remains visible through the views after the modification is committed.
  ENCRYPTION- prevents the view getting published as part of FOR Replication

FUNCTIONS:-
Functions can be used in below places.
  1. In T-SQL statements such as SELECT
  2. In applications calling the function
  3. In the definiton of another user-defined function
  4. To parameterize a view or improve the functionality of an indexed view.
  5. To define a column in a table
  6. To define a check constraint on a column
  7. use an inline function as a filter predicate for a security policy
  8. To replace a stored procedure.
  
  CREATE OR ALTER FUNCTION <function_name> ([
  {@parameter_name datatype [= default] [READONLY] }])
  RETURNS return_data_type | TABLE
  [WITH <<function_optiom>>]
  [AS]
  BEGIN <function_body>
  RETURN <scalar expression>
  END
  
Function can be of three types:-
  1. Scalar function
  2. Inline Table Valued Function - Returns table struct output whithout any computation.
  3. Multi-Statement Table-Valued Functions - Ypu can return your own defined table. It allows computatation
  
Function options can be of follows:-
  1. ENCRYPTION - Converts the function in obfuscated format. User cannot directly access function definition
  2. SCHEMABINDING - When SCHEMABINDING is specified, the base objects cannot be modified in a way that would affect the function definition.
  3. RETURNS NULL ON NULL INPUT | CALLED ON NULL INPUT - CALLED ON NULL INPUT is implied by default. This means that the function body executes even if NULL is passed as an argument.
  4. Execute_As_Clause - 
  5. INLINE = {ON | OFF} - applicable only for scalr user defined functions. used for optimizing query performance

User defined funtions cannot perform following:-
  1. Cannot modify database state
  2. Cannot contain OUTPUT INTO
  3. Cannot return multiple result sets. Use stored proc to return multiple result set
  4. Cannot call stored procedure but can call extended stored procedure.
  5. Error handling is restricted in user defined function
  6. SET function not allowd
  7. FOR XML not allowed
  8. Cannot make use of dynamic SQL or temp tables.
  9. cannot be nested 
  10. The following Service Broker statements cannot be included in the definition of a Transact-SQL user-defined function: BEGIN DIALOG CONVERSATION, END CONVERSATION, GET CONVERSATION GROUP, MOVE CONVERSATION, RECEIVE and SEND

Below are the table options available:-
  1. Data compression = {NONE | ROW | PAGE}
  2. SYSTEM_VERSIONING = ON 
  
Clustered and Non Clustered Index
  1. An index is on-disk structure associated with a table or view that speeds retrieval of rows and columns from a table or view. An index contains keys built from one or more columns in a table or view.
  2. These keys are stored in a structure(B-tree) that enables SQL server on faster retrieval of rows.
  A table or view can contain following type of index:
  Clustered: 
    Clustered indexes sort and store the data rows in the table or view based on their key values. These are the columns included in the index definition. There can be only one clustered index per table, because the data rows themselves can be stored in only one order
    The only time the data rows in a table are stored in sorted order is when the table contains a clustered index. When a table has a clustered index, the table is called a clustered table. If a table has no clustered index, its data rows are stored in an unordered structure called a heap.
  Non- Clustered
    Nonclustered indexes have a structure separate from the data rows. A nonclustered index contains the nonclustered index key values and each key value entry has a pointer to the data row that contains the key value
    The pointer from an index row in a nonclustered index to a data row is called a row locator. The structure of the row locator depends on whether the data pages are stored in a heap or a clustered table. For a heap, a row locator is a pointer to the row. For a clustered table, the row locator is the clustered index key.
    You can add non key columns to the leaf level of the non-clustered index by-pass existing index key limits and execute fully covered, indexed query with the help of INCLUDED OPTION
Both clustered and non clustered index can be unique. This means no two rows can have same column values of index key. This can be achieved using UNIQUE index.
Index and Constraints:- Indexes are automatically created when PRIMARY key and UNIQUE constraints are created.
Query Optimizer:
  Well designed index can reduce I/O operations and consume fewer system resources, therefore improving query performance. 
  When SELECT, UPDATE, MERGE and DELETE query is fired, query optimizer evaluates each available method for retrieving the data and selects the most effiecient method. This method can be table scan, or scanning one or more indexs if exist
  
Clustered Index:
  Every table is recommended to have a clusterd index. Besides query performance, it allows you to rebuilt and re-organize based on the needs. Clustered index can be created on views as well.
  PRIMARY KEY and UNIQUE constraints: When primary key is created, a unique clustered index is created automatically and when UNIQUE constraint is created, then UNIQUE non-clustered index is created.
  Index created as part of constraints will have a constraint same name as column name
  Index independent of constraints: You can create a clustered index on a column other than primary key column if a non clustered primary key is specified.
  Limitation
    There might be need of temporary disk space as we need for transformation from old structure to new structure. Old structure is deallocated after committing full transformation of new structure.
    If a clustered index is created on a heap with several existing nonclustered indexes, all the nonclustered indexes must be rebuilt so that they contain the clustering key value instead of the row identifier (RID). This is applicable when clustered index is dropped.
    We need consider setting ONLINE option ON for large tables. This avoids long term table locks and enables updates and query to underlying table.
    The index key of a clustered index cannot contain varchar columns that have existing data in the ROW_OVERFLOW_DATA allocation unit. 
    If a clustered index is created on a varchar column and the existing data is in the IN_ROW_DATA allocation unit, subsequent insert or update actions on the column that would push the data off-row will fail.
    
Non Clustered Index:
  Generally, non clustered index is used to improve performance of frequently used queries
  The maximum number of index to be created is 999 including indexes created by default constraint field. This does not include XML indexes.
  
  Unique Constraint or Index:
    The behaviour of creating UNIQUE constraint or UNIQUE index is same. Database will not be able to seperate both.
    There is an option can be set which is IGNORE DUPLICATE KEYS. If ignore DUPLICATE key option is set to YES, then when trying to insert row having duplicated value of unique column then adding duplicate row is ignored instead of failing whole of insert operation.
    Unique indexes ensure the data integrity of the defined columns
    Unique indexes provide additional information helpful to the query optimizer that can produce more efficient execution plans.
    
    Indexed view: To create a indexed view, a unique clustered index is defined on one or more view columns. The view is executed and the result set is stored in the leaf level of the index in the same way table data is stored in a clustered index
    Before creating Indexed view, appropriate SET operation should be set. Below are the few SET operations:
    CONCAT_NULL_YIEDS_NULL
    NUMERIC_ROUNDABORT
    ANSI_NULLS - where column=NULL will not yield any records which follows ISO standards.
    ANSI_PADDING - Pad original value (with trailing blanks for char columns and with trailing zeros for binary columns) to the length of the column.
      - Pad original value (with trailing blanks for char columns and with trailing zeros for binary columns) to the length of the column.
    ANSI_WARNING - When set to ON, if null values appear in aggregate functions, such as SUM, AVG, MAX, MIN, STDEV, STDEVP, VAR, VARP, or COUNT, a warning message is generated. When set to OFF, no warning is issued.
      - When set to ON, the divide-by-zero and arithmetic overflow errors cause the statement to be rolled back and an error message is generated. 
    ARITABORT - Terminates the query when an overflow or divide-by-zero error occurs during query execution.
    QUOTED_INDENTIFIER - Allows T-SQL key word to used to create as column name, table name and other identifier when enclosed with double quotes
    
    To create indexed view, ANSI NULL, ANSI PADDING, ANSI WARNING , ANSI ABORT and QUOTED identifier, CONCAT NULL YIELDS NULL option should be ON and NUMERIC_ROUNDABORT option should be off
    Indexed View must be deterministic. This means you cannnot use functions like GETDATE() in the column of index
    IGNORE_DUP_KEY key must be OFF
    Indexed view should be created with Schema binding option.
    Even the function referenced should have been created with WITH SCHEMABINDING
    The view must reference only base tables that are in the same database as the view. The view cannot reference other views.
    EXTERNAL ACCESS = NO
    DATA ACCESS = NO SQL
    PRECISE = TRUE
    DETERMINISTIC = TRUE
    The select statement should not contain COUNT, ROWSET functions, JOINS, DISTINCT, SELF JOiNS, Aggregate functions, PIVOT, OUTER APPLY, CROSS APPLY, Sparse Column set, Inline TVF, multi-valued TVF, OFFSET, CHECKSUM AGG
    If GROUP BY is present, the VIEW definition must contain COUNT_BIG(*) and must not contain HAVING. 
    If the view definition contains a GROUP BY clause, the key of the unique clustered index can reference only the columns specified in the GROUP BY clause.
    Indexed views can be created on a partitioned table, and can themselves be partitioned.
    To prevent the Database Engine from using indexed views, include the OPTION (EXPAND VIEWS) hint on the query. Also, if any of the listed options are incorrectly set, this will prevent the optimizer from using the indexes on the views. For more information about the OPTION (EXPAND VIEWS)
    Dropping the clustered index on the view removes the stored result set, and the optimizer returns to processing the view like a standard view.
    OVER expression should not be used.
    SUM function that references only non-nullable expression can be used.
    
  Filtered Indexes
    A filter index is an optimized non-cluster index especially suited for queries that select from a well suited subset of data.
    A well designed index can improve query performance as well as reduce index maintenence and storage costs compared to full-table indexes.
    Important advantages are :
      1. Improve query performance and plan quality
      2. Reduce indexed costs
      3. Reduce index storage costs
    Design Consideration:
      When a column only has a small number of relevant values for queries, you can create a filtered index on the subset of values. For example, when the values in a column are mostly NULL and the query selects only from the non-NULL values, you can create a filtered index for the non-NULL data rows. The resulting index will be smaller and cost less to maintain than a full-table nonclustered index defined on the same key columns.
      When a table has heterogeneous data rows, you can create a filtered index for one or more categories of data. This can improve the performance of queries on these data rows by narrowing the focus of a query to a specific area of the table. Again, the resulting index will be smaller and cost less to maintain than a full-table nonclustered index.
    Limitation:
      You cannot create filtered index for a view.
      However, the query optimizer can benefit from a filtered index defined on a table that is referenced in a view.
      You cannot create a filtered index on a table when the column accessed in the filter expression is of a CLR data type
      A column in the filtered index expression should be a key or included column in the filtered index definition if the column is in the query result set.
      A column in the filtered index expression does not need to be a key or included column in the filtered index definition if the filtered index expression is equivalent to the query predicate and the query does not return the column in the filtered index expression with the query results.
      A column in the filtered index expression should be a key or included column in the filtered index definition if the query predicate uses the column in a comparison that is not equivalent to the filtered index expression.
      CAST and CONVERT should be happening in the right side of the operator.
    To ensure, SELECT statement must include WITH INDEX options.
    
  Create Indexes with Included columns
    They can be data types not allowed as index key columns.
    They are not considered by the Database Engine when calculating the number of index key columns or index key size.
    An index with nonkey columns can significantly improve query performance when all columns in the query are included in the index either as key or nonkey columns. 
    Design Consideration:
      Redesign nonclustered indexes with a large index key size so that only columns used for searching and lookups are key columns. Make all other columns that cover the query into nonkey columns. In this way, you will have all columns needed to cover the query, but the index key itself is small and efficient.
      Include nonkey columns in a nonclustered index to avoid exceeding the current index size limitations of a maximum of 32 key columns and a maximum index key size of 1,700 bytes (16 key columns and 900 bytes prior to SQL Server 2016 (13.x)). 
    Limitations and Restriction
      Nonkey columns can only be defined on nonclustered indexes.
      All data types except text, ntext, and image can be used as nonkey columns.
      Computed columns that are deterministic and either precise or imprecise can be nonkey columns
      Nonkey columns cannot be changed, except to do the following:
        Change the nullability of the column from NOT NULL to NULL.
        Increase the length of varchar, nvarchar, or varbinary columns.
      Nonkey columns cannot be dropped from a table unless that table's index is dropped first.
      
Drop an Index:-
  DROP INDEX <<Index name>> ON <<Table name>>
Alter an Index:-
  Adding DROP_EXISTING = ON option in WITH allows you to drop and re-create existing index
  ALTER INDEX {<<index name>> | ALL } on <<table name>>
  {
    REBUILD { 
        [PARTITION = ALL | partition name  [ with <<rebuild index option >>] ]
        }
    | DISABLE
    | REORGANIZE [PARTITION = ALL | partition name ] [ with <<re-organize index option>>]
    | SET (SET index option)
    | RESUME [ WITH <<resume index option>>]
    | PAUSE
    | ABORT
  }
  
  rebuild index options can be as follows:-
    PAD_INDEX = {ON | OFF}
    FILL FACTOR
    SORT_IN_TEMPDB = {ON | OFF}
    IGNORE_DUP_KEY = {ON | OFF}
    STATISTICS_NORECOMPUTE = {ON | OFF}
    STATISTICS_INCREMENTAL = {ON | OFF}
    ONLINE = { ON <<low priority wait time>>| OFF }
    RESUMABLE = {ON | OFF}
    MAX DUARTION = <time> [MINUTES]
    ALLOW_ROW_LOCKS = {ON|FF}
    ALLOW_PAGE_LOCKS = {ON|OFF}
    MAXDOP = Max_Degree_Of_Parallelism
    COMPRESSION_DELAY = {0| delay [Minutes]}
    DATA_COMPRESSION = { NONE | ROW | PAGE | COLUMNSTORE | COLUMNSTORE_ARCHIVE }   
        [ ON PARTITIONS ( {<partition_number> [ TO <partition_number>] } [ , ...n ] )
  Above rebuild options works when rebuild all the partition
  When rebuilding single partition, only few option as mentioned below are allowed.
    RESUMABLE = {ON | OFF}
    MAX DUARTION = <time> [MINUTES]
    DATA_COMPRESSION = { NONE | ROW | PAGE | COLUMNSTORE | COLUMNSTORE_ARCHIVE }   
    SORT_IN_TEMPDB = {ON | OFF}
    ONLINE = { ON <<low priority wait time>>| OFF }
    
  <reorganize_option>::=  
    {  
           LOB_COMPACTION = { ON | OFF }  
        |  COMPRESS_ALL_ROW_GROUPS =  { ON | OFF}  
    }  
    
  <set_index_option>::=  
    {  
          ALLOW_ROW_LOCKS = { ON | OFF }  
        | ALLOW_PAGE_LOCKS = { ON | OFF }  
        | IGNORE_DUP_KEY = { ON | OFF }  
        | STATISTICS_NORECOMPUTE = { ON | OFF }  
        | COMPRESSION_DELAY= {0 | delay [Minutes]}  
    } 
  <resumable_index_option> ::=
   { 
      MAXDOP = max_degree_of_parallelism
      | MAX_DURATION =<time> [MINUTES]
      | <low_priority_lock_wait>  
   }
 <low_priority_lock_wait>::=  
    {  
        WAIT_AT_LOW_PRIORITY ( MAX_DURATION = <time> [ MINUTES ] ,   
                              ABORT_AFTER_WAIT = { NONE | SELF | BLOCKERS } )  
    } 

If the table has one or more Spatial Index, XML index or Column store index, following options will
  REBUILD WITH ONLINE =ON
  IGNORE_DUP_KEY=ON
  ONLINE=ON
Other than that, rebuild partition(ALL) fails, for non-partitioned index
REORGANIZE fails when ALLOW_PAGE_LOCKS is off
REORGANIZE partition fails for non-partitioned index
RESUMABLE =ON is not suppported when partition=ALL is found

While including option ALL for REBUILD of clustered index, it rebuilds all the non-clustered index

LOB_COMPACTION= ON
  Specifies to compact all pages that contain data of these large object (LOB) data types: image, text, ntext, varchar(max), nvarchar(max), varbinary(max), and xml. Compacting this data can reduce the data size on disk
  For a clustered index, this compacts all LOB columns that are contained in the table.
  For a nonclustered index, this compacts all LOB columns that are nonkey (included) columns in the index.
For columnstore indexes, REORGANIZE compresses each CLOSED delta rowgroup into the columnstore as a compressed rowgroup. The REORGANIZE operation is always performed online. 
FOR PAD_INDEX ON option:-
   The percentage of free space that is specified by FILLFACTOR is applied to the intermediate-level pages of the index. If FILLFACTOR is not specified at the same time PAD_INDEX is set to ON, the fill factor value stored in sys.indexes is used.
FOR PAD_INDEX OFF option:-
  The intermediate-level pages are filled to near capacity. This leaves sufficient space for at least one row of the maximum size that the index can have, based on the set of keys on the intermediate pages.
SORT_IN_TEMPDB = ON
  The intermediate sort results that are used to build the index are stored in tempdb. If tempdb is on a different set of disks than the user database, this may reduce the time needed to create an index. However, this increases the amount of disk space that is used during the index build.
SORT_IN_TEMPDB = OFF
  The intermediate sort results are stored in the same database as the index.If a sort operation is not required, or if the sort can be performed in memory, the SORT_IN_TEMPDB option is ignored.
IGNORE_DUP_KEY cannot be set to ON for indexes created on a view, non-unique indexes, XML indexes, spatial indexes, and filtered indexes.
  STATISTICS_NORECOMPUTE =ON
    Out-of-date statistics are not automatically recomputed.
  STATISTICS_INCREMENTAL = ON
    When ON, the statistics created are per partition statistics. When OFF, the statistics tree is dropped and SQL Server re-computes the statistics. The default is OFF.
  ONLINE = ON
  Specifies whether underlying tables and associated indexes are available for queries and data modification during the index operation. The default is OFF
  When Index being built with ONLINE=OFF option, when concurrent users trying to access data in the underlying table, index, if one user re-builts the index, there will be exclusive lock on underlying data which prevents other users in running update, select or insert statements.
  Online Index Structure
    1. Source and Pre-existing indexes
      When clustered index for an table is being re-built, the existing clustered index along with associated non-clustered index will be available for query and modifying data.  This means they may be selected by the query optimizer and, if necessary, specified in index hints.
    2. Target
      Target state is new clustered index created. The target index is not searched while processing SELECT statements until the index operation is committed. Internally, the index is marked as write-only. Until the associated non-cluster index is re-built, new index will not be used for search statements in SQL Server
    3. Temporary Mapping Index
      When the index being re-built, a temporary mapping index is created as when concurrent transactions occurs, it stores to determine the records to be modified in new index.
    All ONLINE operation will have three phases such as Preparation, Build and Final
  When you perform online index operations, the following guidelines apply,
    Clustered indexes must be created, rebuilt, or dropped offline when the underlying table contains the following large object (LOB) data types: image, ntext, and text.
    Nonunique nonclustered indexes can be created online when the table contains LOB data types but none of these columns are used in the index definition as either key or nonkey (included) columns.
    Indexes on local temp tables cannot be created, rebuilt, or dropped online. This restriction does not apply to indexes on global temp tables.
    Indexes can be resumed from where it stopped after an unexpected failure, database failover, or a PAUSE command.
  RESUMABLE = { ON | OFF}
    Indicates whether online index is resumable.
  ALLOW_PAGE_LOCKS = ON
    Specifies whether page lock is allowed
  ALLOW_ROW_LOCKS = ON
    Specifies whether row lock is allowed.
  MAXDOP
    Overrides the number of processor used in parallel plan execution
  COMPRESSIONDELAY
    For a disk-based table, delay specifies the minimum number of minutes a delta rowgroup in the CLOSED state must remain in the delta rowgroup before SQL Server can compress it into the compressed rowgroup. Since disk-based tables don't track insert and update times on individual rows, SQL Server applies the delay to delta rowgroups in the CLOSED state.
  DATA_COMPRESSION
    Specifies the Data compression option for specified index, partition number, or range of operations.
  COLUMNSTORE_ARCHIEVE
    Further compresses the specified partition to a smaller size. This can be used for archival, or for other situations that require a smaller size and can afford more time for storage and retrival.
  
  DATA_COMPRESSION - Specifies the partitions to which the DATA_COMPRESSION settings applies. If ON PARTITIONS is not specified, then DATA_COMPRESSION is applied for all partitions.
  ONLINE = ON -> Specifies whether an index or index partition of an underlying table can be rebuilt online or offline. The default option is OFF.
  WAIT_AT_LOW_PRIORITY -> This comes with two options. MAX_DUARTION = <time> [MINUTES] , ABORT_AFTER_WAIT = {NONE|SELF|BLOCKERS}
  Omitting the WAIT AT LOW PRIORITY option is equivalent to WAIT_AT_LOW_PRIORITY (MAX_DURATION = 0 minutes, ABORT_AFTER_WAIT = NONE
  SELF - Exits the online index rebuild DDL operation currently being executed without taking any actions.
  Kill all user transactions that block the online index rebuild DDL operation so that the operation can continue. 
  
  RESUME - Resume an index that is manually or due to failure.
  MAX_DURATION used with RESUMABLE=ON - The time (an integer value specified in minutes) the resumable online index operation is executed after being resumed. Once the time expires, the resumable operation is paused if it is still running.
  WAIT_AT_LOW_PRIORITY used with RESUMABLE=ON and ONLINE = ON.
    Resuming an online index rebuild after a pause has to wait for blocking operations on this table. WAIT_AT_LOW_PRIORITY indicates that the online index rebuild operation will wait for low priority locks, allowing other operations to proceed while the online index build operation is waiting. 
  
  PAUSE - Pause an online index operation
  ABORT - Abort an running or paused index operation that was declared as resumable.
  
  Rebuilding Index:-
    Rebuilding an index drops and re-creates the index. This removes fragmentation, reclaims disk space by compacting the pages based on the specified or existing fill factor setting, and reorders the index rows in contiguous pages. 
    
You can also move index from one file group to another file group using  DROP_EXISTING=ON option
Index on Computed Column:
  You can define indexes on computed columns as long as the following requirement are met:
    Ownership requirements
    Determinism requirements - Column needs to be deterministic and PERSISTED
    Precision requirements - It is not an expression of float and real data types.
    Data type requirements - It cannot evaluate image, ntext, text, varchar(max), nvarchar(max)
    SET OPTION requirements - NUMBERIC_ROUNDABORT is set off. ANSI_NULLS, ANSI_PADDINGS, ANSI_WARNINGS, ARITHABORT, CONCAT_NULL_YIELDS_NULL, QUOTED_IDENTIFIER
    
  SORT_IN_TEMPDB
    When this option is ON, you can use tempdb to store the immediate sort results that are used to build the index. This increases amount of tempdb space. But this decrease the amount of 
    
    Disbaling Index or constraint prevents users access to the index, for clustered indexs to the underlying table data. The index definition remains in metadata, and index statistics are kept on nonclustered indexes. Diabling a nonclustered or clustered index on a view, physically deletes the index data. Disabling a clustered index on a table prevents access to the data; the data still remains in the table, but unavailable for DML operation.
    Index is not maintained when diabled.
    When disabling a unique index, the PRIMARY or UNIQUE constraint, then all the foreign key referenced the index columns from the other table is also disabled.
    Non clustered index are automatically disbaled when relavent clustered index is disabled.
    Nonclustered indexes must be explicitly enabled, unless the clustered index was enabled by using the ALTER INDEX ALL REBUILD statement.
    ALTER INDEX ALL REBUILD command rebuilds and enables all the index for the index, except for the INDEX of the table.
    Disabling a clustered index on a table also disables all clustered and nonclustered indexes on views that reference that table. These indexes must be rebuilt just as those on the referenced table.
    You can rebuild the non-clustered index, only when clustered index for that table is enabled.
    The CREATE_STATISTIC statement cannot be successfully executed on a table that has a disabled clustered index.
    The AUTO_CREATE_STATISTICS database option creates new statistics on a column when the index is disabled and the following condition exists:
      AUTO_CREATE_STATISTICS is set to ON
      There are no existing statistics for the column.
      Statistics are required during query optimizer.
      
    You can rebuild index using follwing options:-
      1. REBUILD
      2. CREATE INDEX using DROP_EXISTING=ON
      3. DBCC DBREINDEX("SchemaName.TableName", IndexName) , DBCC DBREINDEX("SchemaName.TableName", " ") -> rebuilds all the index.
      
    Executing sp_rename procedure will help us to rename the index object.
    Syntax: EXEC sp_rename <old_index_name>, <new_index_name>
    
    Index Operation that require no additional disk space.
      ALTER INDEX REORGANIZE. However log space is required.
      DROP INDEX when you are dropping a nonclustered index
      DROP INDEX when you are dropping a clustered index offline.
      CREATE TABLE (PRIMARY KEY or UNIQUE constraint)
    
    Index Operations that require additional disk space
      CREATE INDEX
      CREATE INDEX WITH DROP_EXISTING
      ALTER INDEX REBUILD
      ALTER TABLE ADD CONSTRAINT (PRIMARY KEY OR CONSTRAINT)
      ALTER TABLE DROP CONSTRAINT (PRIMARY KEY OR CONSTRAINT)
      DROP INDEX MOVE TO
      
    Similarly, when creating index with ONLINE option.
    
  Transaction Log Disk space for Index Operations
    Large scale index operations can generate large data loads that can cause the transaction log to fill quickly.
    To make sure that the index operation can be rolled back, transaction log cannot be truncated until the index operation is completed.
    Transaction log must have sufficient room to store both the index operation transactions and any concurrent users transactions for the duration of the index operations
    This holds true for both online and offline index.
    Offline index transaction log will have detail of only index operation
    Online index transaction log will have details of both index operation and concurrent users transactions
    SORT_IN_TEMPDB option will help to reduce the space of transaction log details. When SORT_IN_TEMPDB option is used, transaction details are stored in tempdb transaction log and concurrent user transaction details will be stored in user database transaction logs.
    
  Reorganize and Rebuild
    SQL server automatically modifies indexes whenever insert, update or delete operations are made to underlying data. 
    Overtime, these modifications can cause the information in the index become scattered in a database(fragmented).
    Fragmentation exists when the index have pages in which the logical ordering, based on the key value, does not match the physical ordering inside the data file.
    Heavily fragmented indexes can degrade query performance and cause the application to respond slowly, espacially scan operation.
    You can remedy index fragmentation by reorganising and rebuilding the index.
    Rebuilding an index drops and re-creates the index. This removes fragmentation, reclaims disk space by compacting the pages based on the specified or existing fill factor setting, and reorders the index rows in contiguous pages. 
    Reorganizing an index uses minimal system resources. It defragments the leaf level of clustered and nonclustered indexes on tables and views by physically reordering the leaf-level pages to match the logical, left to right, order of the leaf nodes. Reorganizing also compacts the index pages. 
    You can system function sys.dm_db_index_physical_stats to analyse the degree of fragmentation.
    If avg_fragmentation_in_percent value is between 5% and 30%, then "ALTER INDEX REORGANIZE" statement can be used.
    If avg_fragmentation_in_percent value is greater than 30%, then "ALTER INDEX REBUID WITH (ONLINE=ON)" option to be used.
    
  The fill-factor option is provided for fine-tuning index data storage and performance. When an index is created or rebuilt, the fill-factor value determines the percentage of space on each leaf-level page to be filled with data, reserving the remainder on each page as free space for future growth. For example, specifying a fill-factor value of 80 means that 20 percent of each leaf-level page will be left empty, providing space for index expansion as data is added to the underlying table. The empty space is reserved between the index rows rather than at the end of the index.
  
  On multiprocessor computers that are running SQL Server Enterprise or higher, index statements may use multiple processors to perform the scan, sort, and index operations associated with the index statement just like other queries do.
  The number of processors used to run a single index statement is determined by the max degree of parallelism configuration option, the current workload, and the index statistics. The max degree of parallelism option determines the maximum number of processors to use in parallel plan execution.
  If the SQL Server Database Engine detects that the system is busy, the degree of parallelism of the index operation is automatically reduced before statement execution starts.
  The Database Engine can also reduce the degree of parallelism if the leading key column of a non-partitioned index has a limited number of distinct values or the frequency of each distinct value varies significantly.

Operation performed by columnstore on rowgroups and column segments
  1. Compress rowgroups into the columnstore. Compression is performed on each column segment into a rowgroup.
  2. Merge rowgroups during an ALTER INDEX.. REORGANISE operation
  3. Creates row groups during an ALTER INDEX.. REBUILD operation
  4. Reports rowgroup health and fragmentation in the dynamic management views (DMVs)
  
Each column has some of its values in each rowgroup. These values are called column segments. Each row contains one column segements for every column in the table. Each column has one column segment in each rowgroup.

  Rows go to the deltastore when they are:

    Inserted with the INSERT INTO ... VALUES statement.
    At the end of a bulk load and they number less than 102,400.
    Updated. Each update is implemented as a delete and an insert.
  The deltastore also stores a list of IDs for deleted rows that have been marked as deleted but not yet physically deleted from the columnstore.
  You can force delta rowgroups into the columnstore by using ALTER INDEX to rebuild or reorganize the index.
  You can create an updatable nonclustered columnstore index on a rowstore table. The columnstore index stores a copy of the data so you do need extra storage. However, the data in the columnstore index will compress to a smaller size than the rowstore table requires. By doing this, you can run analytics on the columnstore index and transactions on the rowstore index at the same time. The column store is updated when data changes in the rowstore table, so both indexes are working against the same data.
  
  Column Store Design
    ->Best for dataware housing analytics
    ->Best if updates and deletes are less
    ->Not suited for OLTP
    
 Choose the best columnstore index for your needs:-
  A columnstore index is either clustered or nonclustered. A clustered columnstore index can have one or more nonclustered B-tree indexes.
  If you create a table as a columnstore index, you can easily convert the table back to a rowstore table by dropping the columnstore index.
  
Recommendations:-
  Clustered Columnstore Index:
    Use for:
      1) Traditional data warehouse workload with a star or snowflake schema
      2) Internet of Things (IOT) workloads that insert large volumes of data with minimal updates and deletes.
   Non-clustered B-tree Index and Columnstore index in an in-memory table
    Use to:
      1. Enforce primary key and foreign key constraints on a clustered columnstore index.
      2. Speed up queries that search for specific values or small ranges of values.
      3. Speed up updates and deletes of specific rows.
   Nonclustered columnstore index on a disk-based heap or B-tree index
      Use for: 
        1) An OLTP workload that has some analytics queries. You can drop B-tree indexes created for analytics and replace them with one nonclustered columnstore index.
        2) Many traditional OLTP workloads that perform Extract Transform and Load (ETL) operations to move data to a separate data warehouse. You can eliminate ETL and a separate data warehouse by creating a nonclustered columnstore index on some of the OLTP tables.
   
   Columnstore index is used in IOT where you have large amount of inserts and minimal deletes and updates.
   
   Dont use clustered columnstore index for below scenarios:-
    The table requires  varchar(max), nvarchar(max), or varbinary(max) data types
    The table data is not permanent. Consider using a heap or temporary table when you need to store and delete the data quickly.
    The table has less than one million rows per partition.
    More than 10% of the operations on the table are updates and deletes. Large numbers of updates and deletes cause fragmentation. The fragmentation affects compression rates and query performance until you run an operation called reorganize that forces all data into the columnstore and removes fragmentation.
    
    Both rowstore and columnstore tables use partition to:
      Control the size of incremental backups. You can back-up partition to seperate filegroups and mark them as read-only.
      Save storage costs by moving an older partition to less expensive storage.
      Perform operations efficiently by limiting the operations to a partition.
      Save an additional 30% in storage costs. You can compress older partitions with the COLUMNSTORE_ARCHIVE compression options. The data will be slower for query performance, which is acceptable if the partition is queries infrequently.
      
      Use MAXDOP to improve rowgroup quality:
        When converting B-tree index to columnstore index and involving bulk of data, you can increase the MAX DOP value to 4 to speed up the process.
      Use DROP_EXISTING
        helps to preserve the sort order while converting from rowindex to columnindex
     
